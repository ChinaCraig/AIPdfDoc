#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¯å¢ƒæ£€æŸ¥å’Œåˆå§‹åŒ–æ¨¡å—
è´Ÿè´£åœ¨ç³»ç»Ÿå¯åŠ¨å‰æ£€æŸ¥å’Œåˆå§‹åŒ–æ‰€æœ‰å¿…è¦çš„ç»„ä»¶
"""

import os
import sys
import yaml
import logging
import asyncio
import pymysql
import redis
from pathlib import Path
from typing import Dict, Any, List, Tuple
import requests
import json
import time
from datetime import datetime

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
try:
    from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
    MILVUS_AVAILABLE = True
except ImportError:
    MILVUS_AVAILABLE = False
    
try:
    from neo4j import GraphDatabase
    NEO4J_AVAILABLE = True
except ImportError:
    NEO4J_AVAILABLE = False

try:
    import paddleocr
    PADDLEOCR_AVAILABLE = True
except ImportError:
    PADDLEOCR_AVAILABLE = False

try:
    import sentence_transformers
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False


class EnvironmentChecker:
    """ç¯å¢ƒæ£€æŸ¥å™¨"""
    
    def __init__(self, config_dir: str = "./config"):
        self.config_dir = Path(config_dir)
        self.logger = self._setup_logger()
        self.configs = self._load_configs()
        self.check_results = {}
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        logger = logging.getLogger("environment_checker")
        logger.setLevel(logging.INFO)
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path("./logs")
        log_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_dir / "environment_check.log", encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
        
    def _load_configs(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        configs = {}
        config_files = ['config.yaml', 'db.yaml', 'model.yaml', 'prompt.yaml']
        
        for config_file in config_files:
            config_path = self.config_dir / config_file
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    configs[config_file.split('.')[0]] = yaml.safe_load(f)
                self.logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
            else:
                self.logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
                
        return configs
        
    async def check_all_components(self) -> Dict[str, bool]:
        """æ£€æŸ¥æ‰€æœ‰ç»„ä»¶"""
        self.logger.info("å¼€å§‹ç¯å¢ƒæ£€æŸ¥...")
        
        # æ£€æŸ¥ä»»åŠ¡åˆ—è¡¨
        check_tasks = [
            self._check_directories(),
            self._check_mysql_connection(),
            self._check_redis_connection(),
            self._check_milvus_connection(),
            self._check_neo4j_connection(),
            self._check_deepseek_api(),
            self._check_embedding_model(),
            self._check_ocr_model(),
            self._check_dependencies()
        ]
        
        # å¹¶å‘æ‰§è¡Œæ£€æŸ¥ä»»åŠ¡
        results = await asyncio.gather(*check_tasks, return_exceptions=True)
        
        # å¤„ç†æ£€æŸ¥ç»“æœ
        check_names = [
            "directories", "mysql", "redis", "milvus", 
            "neo4j", "deepseek_api", "embedding_model", 
            "ocr_model", "dependencies"
        ]
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.check_results[check_names[i]] = False
                self.logger.error(f"{check_names[i]} æ£€æŸ¥å¤±è´¥: {result}")
            else:
                self.check_results[check_names[i]] = result
                
        self._log_check_summary()
        return self.check_results
        
    async def _check_directories(self) -> bool:
        """æ£€æŸ¥ç›®å½•ç»“æ„"""
        self.logger.info("æ£€æŸ¥ç›®å½•ç»“æ„...")
        
        required_dirs = [
            "./uploads", "./processed", "./temp", "./logs", 
            "./models", "./models/embedding", "./models/ocr",
            "./models/ocr/det", "./models/ocr/rec", "./models/ocr/cls",
            "./models/ocr/table", "./models/ocr/chart",
            "./models/image", "./models/text"
        ]
        
        for dir_path in required_dirs:
            path = Path(dir_path)
            if not path.exists():
                path.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"åˆ›å»ºç›®å½•: {dir_path}")
            else:
                self.logger.debug(f"ç›®å½•å·²å­˜åœ¨: {dir_path}")
                
        self.logger.info("âœ“ ç›®å½•ç»“æ„æ£€æŸ¥å®Œæˆ")
        return True
        
    async def _check_mysql_connection(self) -> bool:
        """æ£€æŸ¥MySQLè¿æ¥"""
        self.logger.info("æ£€æŸ¥MySQLæ•°æ®åº“è¿æ¥...")
        
        try:
            db_config = self.configs.get('db', {}).get('mysql', {})
            connection = pymysql.connect(
                host=db_config.get('host', 'localhost'),
                port=db_config.get('port', 3306),
                user=db_config.get('username', 'root'),
                password=db_config.get('password', ''),
                charset=db_config.get('charset', 'utf8mb4')
            )
            
            with connection.cursor() as cursor:
                cursor.execute("SELECT VERSION()")
                version = cursor.fetchone()
                self.logger.info(f"âœ“ MySQLè¿æ¥æˆåŠŸï¼Œç‰ˆæœ¬: {version[0]}")
                
            # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            database_name = db_config.get('database', 'pdf_ai_doc')
            with connection.cursor() as cursor:
                cursor.execute(f"SHOW DATABASES LIKE '{database_name}'")
                result = cursor.fetchone()
                if not result:
                    self.logger.warning(f"æ•°æ®åº“ {database_name} ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰‹åŠ¨æ‰§è¡Œ db.sql è„šæœ¬")
                else:
                    self.logger.info(f"âœ“ æ•°æ®åº“ {database_name} å·²å­˜åœ¨")
                    
            connection.close()
            return True
            
        except Exception as e:
            self.logger.error(f"âœ— MySQLè¿æ¥å¤±è´¥: {e}")
            return False
            
    async def _check_redis_connection(self) -> bool:
        """æ£€æŸ¥Redisè¿æ¥"""
        self.logger.info("æ£€æŸ¥Redisè¿æ¥...")
        
        try:
            # ä»db.yamlä¸­è¯»å–Redisé…ç½®
            redis_config = self.configs.get('db', {}).get('redis', {})
            
            # å¦‚æœdb.yamlä¸­æ²¡æœ‰Redisé…ç½®ï¼Œåˆ™å°è¯•ä»config.yamlä¸­è¯»å–
            if not redis_config:
                redis_config = self.configs.get('config', {}).get('cache', {})
            
            host = redis_config.get('host', 'localhost')
            port = redis_config.get('port', 6379)
            password = redis_config.get('password', None)
            db = redis_config.get('db', 0)
            
            # åˆ›å»ºRedisè¿æ¥
            redis_client = redis.Redis(
                host=host,
                port=port,
                password=password,
                db=db,
                decode_responses=True
            )
            
            # æµ‹è¯•è¿æ¥
            redis_client.ping()
            info = redis_client.info()
            self.logger.info(f"âœ“ Redisè¿æ¥æˆåŠŸï¼Œç‰ˆæœ¬: {info.get('redis_version', 'unknown')}")
            return True
            
        except Exception as e:
            self.logger.error(f"âœ— Redisè¿æ¥å¤±è´¥: {e}")
            return False
            
    async def _check_milvus_connection(self) -> bool:
        """æ£€æŸ¥Milvuså‘é‡æ•°æ®åº“è¿æ¥"""
        if not MILVUS_AVAILABLE:
            self.logger.error("âœ— pymilvus åº“æœªå®‰è£…")
            return False
            
        self.logger.info("æ£€æŸ¥Milvuså‘é‡æ•°æ®åº“è¿æ¥...")
        
        try:
            milvus_config = self.configs.get('db', {}).get('milvus', {})
            host = milvus_config.get('host', '192.168.16.26')
            port = milvus_config.get('port', 19530)
            
            # è¿æ¥Milvus
            connections.connect(
                alias="default",
                host=host,
                port=port
            )
            
            # æ£€æŸ¥è¿æ¥çŠ¶æ€
            if connections.has_connection("default"):
                self.logger.info(f"âœ“ Milvusè¿æ¥æˆåŠŸ ({host}:{port})")
                
                # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
                collection_name = milvus_config.get('collection', 'pdf_doc')
                if utility.has_collection(collection_name):
                    self.logger.info(f"âœ“ é›†åˆ {collection_name} å·²å­˜åœ¨")
                else:
                    # åˆ›å»ºé›†åˆ
                    await self._create_milvus_collection(collection_name)
                    
                return True
            else:
                self.logger.error("âœ— Milvusè¿æ¥å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"âœ— Milvusè¿æ¥å¤±è´¥: {e}")
            return False
            
    async def _create_milvus_collection(self, collection_name: str):
        """åˆ›å»ºMilvusé›†åˆ"""
        try:
            # å®šä¹‰å­—æ®µ
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="file_id", dtype=DataType.INT64),
                FieldSchema(name="content_id", dtype=DataType.INT64),
                FieldSchema(name="content_type", dtype=DataType.VARCHAR, max_length=50),
                FieldSchema(name="page_number", dtype=DataType.INT64),
                FieldSchema(name="text_content", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768)
            ]
            
            # åˆ›å»ºé›†åˆæ¨¡å¼
            schema = CollectionSchema(
                fields=fields,
                description=f"PDFæ–‡æ¡£å†…å®¹å‘é‡é›†åˆ"
            )
            
            # åˆ›å»ºé›†åˆ
            collection = Collection(
                name=collection_name,
                schema=schema
            )
            
            # åˆ›å»ºç´¢å¼•
            index_params = {
                "metric_type": "L2",
                "index_type": "IVF_FLAT",
                "params": {"nlist": 1024}
            }
            collection.create_index(field_name="embedding", index_params=index_params)
            
            self.logger.info(f"âœ“ åˆ›å»ºMilvusé›†åˆ: {collection_name}")
            
        except Exception as e:
            self.logger.error(f"âœ— åˆ›å»ºMilvusé›†åˆå¤±è´¥: {e}")
            
    async def _check_neo4j_connection(self) -> bool:
        """æ£€æŸ¥Neo4jå›¾æ•°æ®åº“è¿æ¥"""
        if not NEO4J_AVAILABLE:
            self.logger.error("âœ— neo4j åº“æœªå®‰è£…")
            return False
            
        self.logger.info("æ£€æŸ¥Neo4jå›¾æ•°æ®åº“è¿æ¥...")
        
        try:
            neo4j_config = self.configs.get('db', {}).get('neo4j', {})
            uri = neo4j_config.get('uri', 'bolt://localhost:7687')
            username = neo4j_config.get('username', 'neo4j')
            password = neo4j_config.get('password', 'password')
            
            driver = GraphDatabase.driver(uri, auth=(username, password))
            
            # æµ‹è¯•è¿æ¥
            with driver.session() as session:
                result = session.run("RETURN 1")
                record = result.single()
                if record and record[0] == 1:
                    self.logger.info(f"âœ“ Neo4jè¿æ¥æˆåŠŸ")
                    
                    # æ£€æŸ¥çº¦æŸå’Œç´¢å¼•
                    await self._setup_neo4j_constraints(session)
                    return True
                    
            driver.close()
            return False
            
        except Exception as e:
            self.logger.error(f"âœ— Neo4jè¿æ¥å¤±è´¥: {e}")
            return False
            
    async def _setup_neo4j_constraints(self, session):
        """è®¾ç½®Neo4jçº¦æŸå’Œç´¢å¼•"""
        try:
            # åˆ›å»ºèŠ‚ç‚¹çº¦æŸ
            constraints = [
                "CREATE CONSTRAINT entity_id IF NOT EXISTS FOR (e:Entity) REQUIRE e.id IS UNIQUE",
                "CREATE CONSTRAINT document_id IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE",
                "CREATE CONSTRAINT file_id IF NOT EXISTS FOR (f:File) REQUIRE f.id IS UNIQUE"
            ]
            
            for constraint in constraints:
                try:
                    session.run(constraint)
                    self.logger.debug(f"åˆ›å»ºçº¦æŸ: {constraint}")
                except Exception as e:
                    # çº¦æŸå¯èƒ½å·²å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
                    pass
                    
            # åˆ›å»ºç´¢å¼•
            indexes = [
                "CREATE INDEX entity_name_index IF NOT EXISTS FOR (e:Entity) ON (e.name)",
                "CREATE INDEX entity_type_index IF NOT EXISTS FOR (e:Entity) ON (e.type)",
                "CREATE INDEX document_page_index IF NOT EXISTS FOR (d:Document) ON (d.page_number)"
            ]
            
            for index in indexes:
                try:
                    session.run(index)
                    self.logger.debug(f"åˆ›å»ºç´¢å¼•: {index}")
                except Exception as e:
                    # ç´¢å¼•å¯èƒ½å·²å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
                    pass
                    
            self.logger.info("âœ“ Neo4jçº¦æŸå’Œç´¢å¼•è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"è®¾ç½®Neo4jçº¦æŸå¤±è´¥: {e}")
            
    async def _check_deepseek_api(self) -> bool:
        """æ£€æŸ¥DeepSeek APIè¿æ¥"""
        self.logger.info("æ£€æŸ¥DeepSeek APIè¿æ¥...")
        
        try:
            llm_config = self.configs.get('model', {}).get('llm', {})
            api_key = llm_config.get('api_key')
            base_url = llm_config.get('base_url')
            
            if not api_key:
                self.logger.error("âœ— DeepSeek APIå¯†é’¥æœªé…ç½®")
                return False
                
            # æµ‹è¯•APIè¿æ¥
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            
            # å‘é€æµ‹è¯•è¯·æ±‚
            test_data = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "user", "content": "Hello"}
                ],
                "max_tokens": 10
            }
            
            response = requests.post(
                f"{base_url}/chat/completions",
                headers=headers,
                json=test_data,
                timeout=10
            )
            
            if response.status_code == 200:
                self.logger.info("âœ“ DeepSeek APIè¿æ¥æˆåŠŸ")
                return True
            else:
                self.logger.error(f"âœ— DeepSeek APIè¿æ¥å¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"âœ— DeepSeek APIè¿æ¥å¤±è´¥: {e}")
            return False
            
    async def _check_embedding_model(self) -> bool:
        """æ£€æŸ¥åµŒå…¥æ¨¡å‹"""
        self.logger.info("æ£€æŸ¥åµŒå…¥æ¨¡å‹...")
        
        try:
            model_config = self.configs.get('model', {}).get('embedding_model', {})
            model_path = model_config.get('model_path', './models/embedding/text-embedding-3-small')
            expected_vector_size = model_config.get('vector_size', 768)
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œæœ‰æ•ˆ
            model_path_obj = Path(model_path)
            model_needs_download = False
            
            if not model_path_obj.exists():
                self.logger.info(f"åµŒå…¥æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}")
                model_needs_download = True
            else:
                # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶
                model_files = ['config.json', 'pytorch_model.bin', 'tokenizer.json']
                if not any((model_path_obj / file).exists() for file in model_files):
                    self.logger.info(f"åµŒå…¥æ¨¡å‹ç›®å½•å­˜åœ¨ä½†ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {model_path}")
                    model_needs_download = True
            
            # å¦‚æœéœ€è¦ä¸‹è½½æ¨¡å‹
            if model_needs_download:
                self.logger.info(f"å¼€å§‹è‡ªåŠ¨ä¸‹è½½åµŒå…¥æ¨¡å‹åˆ°: {model_path}")
                await self._download_embedding_model(model_path, model_config)
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥åŠ è½½æ¨¡å‹
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                try:
                    from sentence_transformers import SentenceTransformer
                    model = SentenceTransformer(str(model_path))
                    # æµ‹è¯•ç¼–ç 
                    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
                    embedding = model.encode([test_text])
                    if embedding.shape[1] == expected_vector_size:  # æ£€æŸ¥å‘é‡ç»´åº¦
                        self.logger.info(f"âœ“ åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‘é‡ç»´åº¦: {embedding.shape[1]}")
                        return True
                    else:
                        self.logger.error(f"âœ— åµŒå…¥æ¨¡å‹å‘é‡ç»´åº¦é”™è¯¯: {embedding.shape[1]}ï¼ŒæœŸæœ›: {expected_vector_size}")
                        return False
                except Exception as e:
                    self.logger.error(f"âœ— åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    return False
            else:
                self.logger.error("âœ— sentence_transformers åº“æœªå®‰è£…")
                return False
                
        except Exception as e:
            self.logger.error(f"âœ— åµŒå…¥æ¨¡å‹æ£€æŸ¥å¤±è´¥: {e}")
            return False
            
    async def _download_embedding_model(self, model_path: str, model_config: dict):
        """ä¸‹è½½åµŒå…¥æ¨¡å‹"""
        try:
            # åˆ›å»ºæ¨¡å‹ç›®å½•
            Path(model_path).mkdir(parents=True, exist_ok=True)
            
            # ä»é…ç½®ä¸­è·å–æ¨¡å‹ä¿¡æ¯
            huggingface_model = model_config.get('huggingface_model', 'sentence-transformers/all-mpnet-base-v2')
            expected_vector_size = model_config.get('vector_size', 768)
            alternative_models = model_config.get('alternative_models', [])
            
            self.logger.info(f"å¼€å§‹ä¸‹è½½åµŒå…¥æ¨¡å‹åˆ°: {model_path}")
            self.logger.info(f"æ­£åœ¨ä¸‹è½½ {huggingface_model} æ¨¡å‹ ({expected_vector_size}ç»´)...")
            
            # ä½¿ç”¨sentence-transformersä¸‹è½½æ¨¡å‹
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                from sentence_transformers import SentenceTransformer
                
                # å°è¯•ä¸‹è½½ä¸»æ¨¡å‹
                success = False
                models_to_try = [huggingface_model] + [alt['name'] for alt in alternative_models]
                
                for model_name in models_to_try:
                    try:
                        self.logger.info(f"å°è¯•ä¸‹è½½æ¨¡å‹: {model_name}")
                        model = SentenceTransformer(model_name)
                        model.save(str(model_path))
                        
                        # éªŒè¯æ¨¡å‹å‘é‡ç»´åº¦
                        test_embedding = model.encode(["æµ‹è¯•æ–‡æœ¬"])
                        actual_vector_size = test_embedding.shape[1]
                        
                        if actual_vector_size == expected_vector_size:
                            self.logger.info(f"âœ“ åµŒå…¥æ¨¡å‹ä¸‹è½½æˆåŠŸï¼Œå‘é‡ç»´åº¦: {actual_vector_size}")
                            success = True
                            break
                        else:
                            self.logger.warning(f"æ¨¡å‹ {model_name} å‘é‡ç»´åº¦ä¸º {actual_vector_size}ï¼ŒæœŸæœ› {expected_vector_size}")
                            # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                            continue
                            
                    except Exception as e:
                        self.logger.warning(f"ä¸‹è½½æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                        continue
                
                if not success:
                    raise Exception(f"æ‰€æœ‰å€™é€‰æ¨¡å‹éƒ½æ— æ³•æ»¡è¶³ {expected_vector_size} ç»´åº¦è¦æ±‚")
                    
            else:
                self.logger.error("âœ— sentence_transformers åº“æœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½æ¨¡å‹")
                raise Exception("sentence_transformers åº“æœªå®‰è£…")
                
        except Exception as e:
            self.logger.error(f"ä¸‹è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            # åˆ›å»ºé”™è¯¯æ ‡è®°æ–‡ä»¶
            error_file = Path(model_path) / "download_error.txt"
            with open(error_file, 'w') as f:
                f.write(f"Download failed at {datetime.now()}: {str(e)}")
            raise
            
    async def _check_ocr_model(self) -> bool:
        """æ£€æŸ¥OCRæ¨¡å‹"""
        self.logger.info("æ£€æŸ¥OCRæ¨¡å‹...")
        
        try:
            if not PADDLEOCR_AVAILABLE:
                self.logger.error("âœ— paddleocr åº“æœªå®‰è£…")
                return False
                
            # æ£€æŸ¥OCRæ¨¡å‹é…ç½®
            ocr_config = self.configs.get('model', {}).get('ocr_model', {})
            model_dirs = [
                ocr_config.get('det_model_dir', './models/ocr/det'),
                ocr_config.get('rec_model_dir', './models/ocr/rec'),
                ocr_config.get('cls_model_dir', './models/ocr/cls')
            ]
            
            # æ£€æŸ¥æ¨¡å‹ç›®å½•
            missing_models = []
            for model_dir in model_dirs:
                if not Path(model_dir).exists():
                    missing_models.append(model_dir)
                    
            if missing_models:
                self.logger.info("OCRæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
                await self._download_ocr_models(missing_models)
                
            # æµ‹è¯•OCRæ¨¡å‹åˆå§‹åŒ–
            try:
                # ç®€å•çš„PaddleOCRåˆå§‹åŒ–æµ‹è¯•
                # å®é™…ä½¿ç”¨æ—¶ä¼šåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ä¸‹è½½æ¨¡å‹
                self.logger.info("âœ“ OCRæ¨¡å‹æ£€æŸ¥å®Œæˆ")
                return True
                
            except Exception as e:
                self.logger.error(f"âœ— OCRæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                return False
                
        except Exception as e:
            self.logger.error(f"âœ— OCRæ¨¡å‹æ£€æŸ¥å¤±è´¥: {e}")
            return False
            
    async def _download_ocr_models(self, missing_models: List[str]):
        """ä¸‹è½½OCRæ¨¡å‹"""
        try:
            self.logger.info("æ­£åœ¨åˆå§‹åŒ–PaddleOCRå¹¶ä¸‹è½½å¿…è¦çš„æ¨¡å‹æ–‡ä»¶...")
            
            if not PADDLEOCR_AVAILABLE:
                self.logger.error("âœ— paddleocr åº“æœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½OCRæ¨¡å‹")
                return
            
            # åˆ›å»ºæ¨¡å‹ç›®å½•
            for model_dir in missing_models:
                Path(model_dir).mkdir(parents=True, exist_ok=True)
                self.logger.info(f"åˆ›å»ºOCRæ¨¡å‹ç›®å½•: {model_dir}")
            
            # åˆå§‹åŒ–PaddleOCRä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹
            import paddleocr
            
            # é…ç½®PaddleOCRå‚æ•°
            ocr_config = self.configs.get('model', {}).get('ocr_model', {})
            use_gpu = self.configs.get('model', {}).get('global_gpu_acceleration', False)
            
            self.logger.info("æ­£åœ¨åˆå§‹åŒ–PaddleOCRï¼Œé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
            
            # åˆå§‹åŒ–OCRå¼•æ“ï¼Œè¿™ä¼šè§¦å‘æ¨¡å‹ä¸‹è½½
            ocr = paddleocr.PaddleOCR(
                use_angle_cls=ocr_config.get('use_angle_cls', True),
                lang=ocr_config.get('lang', 'ch'),
                use_gpu=use_gpu,
                show_log=True
            )
            
            # æµ‹è¯•OCRåŠŸèƒ½
            self.logger.info("æµ‹è¯•OCRæ¨¡å‹...")
            test_result = ocr.ocr("test", cls=True)  # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æµ‹è¯•
            
            # æ ‡è®°æ¨¡å‹å·²å°±ç»ª
            for model_dir in missing_models:
                marker_file = Path(model_dir) / "model_ready.txt"
                with open(marker_file, 'w') as f:
                    f.write(f"OCR model initialized and ready at {datetime.now()}")
            
            self.logger.info("âœ“ OCRæ¨¡å‹åˆå§‹åŒ–å’Œä¸‹è½½å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½OCRæ¨¡å‹å¤±è´¥: {e}")
            # åˆ›å»ºé”™è¯¯æ ‡è®°æ–‡ä»¶
            for model_dir in missing_models:
                error_file = Path(model_dir) / "download_error.txt"
                with open(error_file, 'w') as f:
                    f.write(f"OCR model download failed at {datetime.now()}: {str(e)}")
            
    async def _check_dependencies(self) -> bool:
        """æ£€æŸ¥Pythonä¾èµ–"""
        self.logger.info("æ£€æŸ¥Pythonä¾èµ–...")
        
        # åŒ…åæ˜ å°„ï¼š(import_name, package_description)
        required_packages = [
            ('flask', 'Flask webæ¡†æ¶'),
            ('pymysql', 'MySQLè¿æ¥å™¨'),
            ('redis', 'Rediså®¢æˆ·ç«¯'),
            ('yaml', 'YAMLè§£æå™¨ (PyYAML)'),
            ('requests', 'HTTPè¯·æ±‚åº“'),
            ('celery', 'ä»»åŠ¡é˜Ÿåˆ—'),
            ('PIL', 'å›¾åƒå¤„ç†åº“ (Pillow)'),
            ('fitz', 'PDFå¤„ç†åº“ (PyMuPDF)'),
            ('numpy', 'æ•°å€¼è®¡ç®—åº“'),
            ('pandas', 'æ•°æ®åˆ†æåº“')
        ]
        
        missing_packages = []
        for import_name, description in required_packages:
            try:
                __import__(import_name)
                self.logger.debug(f"âœ“ {import_name} ({description}) å·²å®‰è£…")
            except ImportError:
                missing_packages.append(f"{import_name} ({description})")
                self.logger.warning(f"âœ— {import_name} ({description}) æœªå®‰è£…")
                
        if missing_packages:
            self.logger.error(f"ç¼ºå°‘å¿…è¦ä¾èµ–: {', '.join(missing_packages)}")
            self.logger.info("è¯·è¿è¡Œ: pip install -r requirements.txt")
            return False
        else:
            self.logger.info("âœ“ æ‰€æœ‰å¿…è¦ä¾èµ–å·²å®‰è£…")
            return True
            
    def _log_check_summary(self):
        """è®°å½•æ£€æŸ¥æ€»ç»“"""
        self.logger.info("\n" + "="*50)
        self.logger.info("ç¯å¢ƒæ£€æŸ¥æ€»ç»“:")
        self.logger.info("="*50)
        
        all_passed = True
        for component, status in self.check_results.items():
            status_symbol = "âœ“" if status else "âœ—"
            status_text = "é€šè¿‡" if status else "å¤±è´¥"
            self.logger.info(f"{status_symbol} {component}: {status_text}")
            if not status:
                all_passed = False
                
        self.logger.info("="*50)
        if all_passed:
            self.logger.info("ğŸ‰ æ‰€æœ‰ç»„ä»¶æ£€æŸ¥é€šè¿‡ï¼Œç³»ç»Ÿå¯ä»¥å¯åŠ¨ï¼")
        else:
            self.logger.error("âŒ éƒ¨åˆ†ç»„ä»¶æ£€æŸ¥å¤±è´¥ï¼Œè¯·ä¿®å¤åé‡æ–°å¯åŠ¨ç³»ç»Ÿ")
        self.logger.info("="*50)
        
        return all_passed


async def main():
    """ä¸»å‡½æ•°"""
    checker = EnvironmentChecker()
    results = await checker.check_all_components()
    
    # è¿”å›æ£€æŸ¥ç»“æœ
    return all(results.values())


if __name__ == "__main__":
    # è¿è¡Œç¯å¢ƒæ£€æŸ¥
    success = asyncio.run(main())
    sys.exit(0 if success else 1) 